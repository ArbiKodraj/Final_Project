import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt

from matplotlib.lines import Line2D
from sklearn.model_selection import train_test_split
from itertools import product
from sklearn.metrics import (
    mean_absolute_error,
    mean_squared_error,
    explained_variance_score,
    r2_score,
)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.wrappers.scikit_learn import KerasRegressor

# ---------------------------------------------------------------------- Functions & Styling

def highlight_min(s):  
    """ Highlights the minimum in a Series yellow

    Args:
        s (pd.Series): Series that is to be highlighted.
    """
    is_max = s == s.min()
    return ["background-color: yellow" if v else "" for v in is_max]

def first_function(x):
    """``Benchmark function.``

    Args:
        x (int, float): Input.

    Returns:
        float: Output. :math:`e^{-x}`
    """
    return np.exp(-x)

def second_function(x):
    """``Benchmark function.``

    Args:
        x (int, float): Input.

    Returns:
        float: Output. :math:`|x|`
    """
    return abs(x)

def third_function(x):
    """Third benchmark function.

    Args:
        x (int, float): Input.

    Returns:
        float: Output. :math:`sin(Ï€x)`
    """
    return x * np.sin(np.pi * x)

# ---------------------------------------------------------------------- Keras Sequential Method

class SeqMethod:
    """Generate neural network using Keras 

    Objects that approximates benchmark functions ``first_function``, 
    ``second_function``, and ``third_function`` using neural system 
    generated by Keras Sequential object.

    Args:
        func (function): Benchmark function.
        nodes (int): Number of nodes. Defaults to 300.
        epochs (int): Number of epochs. Defaults to 1000.
    """
    def __init__(self, func, nodes=300, epochs=1000):
        """Constructor method.
        """
        self.func = func
        self.nodes = nodes
        self.epochs = epochs

    def seq_init(self, activator_out,  activator_in="relu", n1_layer=20, n2_layer=18, n3_init=False, n3_layer=10):
        """Prepares Sequential model. 

        Args:
            activator_out (str): Activation function for output layer.
            activator_in (str, optional): Activation function for input and hidden layers. 
                Defaults to "relu".
            n1_layer (int, optional): Neuron number in first hidden layer. Defaults to 20.
            n2_layer (int, optional): Neuron number in second hidden layer. Defaults to 18.
            n3_init (bool, optional): Initiliazes third hidden layer if True. 
                Defaults to False.
            n3_layer (int, optional): Neuron number in third hidden layer. Defaults to 10.
        """
        self.model = Sequential()
        self.model.add(
            Dense(
                n1_layer, input_dim=1, activation=activator_in, kernel_initializer="he_uniform"
            )
        )  
        self.model.add(
            Dense(n2_layer, activation=activator_in, kernel_initializer="he_uniform")
        )  
        if n3_init == True:
            self.model.add(
                Dense(n3_layer, activation=activator_in, kernel_initializer="he_uniform")
            )  
        self.model.add(Dense(1, activation=activator_out))
        self.model.compile(optimizer="adam", loss="mae")

    def predict_test(self, a=-1, b=1, ts=0.4):
        """Predicts testings data using build Keras network.

        Args:
            a (int, optional): Lower bound of interval. Defaults to -1.
            b (int, optional): Upper bound of interval. Defaults to 1.
            ts (float, optional): Testing size 1-alpha. Defaults to 0.4.
        """
        X = np.linspace(a, b, self.nodes)
        y = self.func(X)
        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=ts
        )
        callback = EarlyStopping(
            monitor="val_loss", patience=100, restore_best_weights=True
        )
        self.model.fit(
            self.x_train,
            self.y_train,
            validation_data=(self.x_test, self.y_test),
            epochs=self.epochs,
            callbacks=[callback],
            verbose=False,
        )
        self.pred = self.model.predict(self.x_test)

    def return_data(self):
        """Returns training and testing data.

        Returns:
            np.array: Training (x,y) and testing (x,y) data.
        """
        return self.x_train, self.x_test, self.y_train, self.y_test, self.pred

#class _Outcome(SeqMethod):
#    def plt_approximation(self, num):
#        train_marker = Line2D(
#            [],
#            [],
#            color="orange",
#            marker="o",
#            linestyle="None",
#            markersize=4,
#            label="Test Data",
#        )
#        test_marker = Line2D(
#            [],
#            [],
#            color="blue",
#            marker="v",
#            linestyle="None",
#            markersize=4,
#            label="Train Data",
#        )
#        approx_marker = Line2D(
#            [],
#            [],
#            color="red",
#            marker="*",
#            linestyle="None",
#            markersize=4,
#            label="Keras Output",
#        )
#        plt.figure(figsize=(15, 5))
#        plt.plot(self.x_train, self.y_train, "o", ms=3, color="b", label="Training Data")
#        plt.plot(self.x_test, self.y_test, "v", ms=3, color="y", label="Testing Data")
#        plt.plot(self.x_test, self.pred, "*", ms=3, color="r", label="Prediction")
#        plt.xlabel("x")
#        plt.title(f"Figure {num}: Approximation of missing Values")
#        plt.legend(handles=[train_marker, test_marker, approx_marker])
#        plt.grid()
#        plt.show()

#    def return_y_test(self):
#        return self.y_test

#    def return_predict(self):
#        return self.pred

def plt_side_by_side(
    num1,
    num2,
    num3,
    x_train1,
    y_train1,
    x_test1,
    y_test1,
    pred1,
    x_train2,
    y_train2,
    x_test2,
    y_test2,
    pred2,
    x_train3,
    y_train3,
    x_test3,
    y_test3,
    pred3,
):
    """Plots training, testing and predicted data of all three benchmark 
    functions. Visualizes approximation accuracy of the ``SeqMethod``
    object.

    Args:
        num1 (int, float): Number of first figure.
        num2 (int, float): Number of second figure.
        num3 (int, float): Number of third figure.
        x_train1 (np.array): Training data x of first benchmark function.
        y_train1 (np.array): Training data y of first benchmark function.
        x_test1 (np.array): Testing data x of first benchmark function.
        y_test1 (np.array): Testing data y of first benchmark function.
        pred1 (np.array): Predicted data first benchmark function.
        x_train2 (np.array): Training data x of second benchmark function.
        y_train2 (np.array): Training data y of second benchmark function.
        x_test2 (np.array): Testing data x of second benchmark function.
        y_test2 (np.array): Testing data y of second benchmark function.
        pred2 (np.array): Predicted data of second benchmark function.
        x_train3 (np.array): Training data x of third benchmark function.
        y_train3 (np.array): Training data y of third benchmark function.
        x_test3 (np.array): Testing data x of third benchmark function.
        y_test3 (np.array Testing data y of third benchmark function.
        pred3 (np.array): Predicted data of third benchmark function.
    """
    train_marker = Line2D(
        [],
        [],
        color="orange",
        marker="o",
        linestyle="None",
        markersize=4,
        label="Test Data",
    )
    test_marker = Line2D(
        [],
        [],
        color="blue",
        marker="v",
        linestyle="None",
        markersize=4,
        label="Train Data",
    )
    approx_marker = Line2D(
        [],
        [],
        color="red",
        marker="*",
        linestyle="None",
        markersize=4,
        label="Keras Output",
    )
    figure, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))
    ax1.plot(x_train1, y_train1, "o", ms=3, color="b")
    ax1.plot(x_test1, y_test1, "v", ms=3, color="y")
    ax1.plot(x_test1, pred1, "*", ms=3, color="r")
    ax1.set_title(f"Figure {num1}: Approximation of " + "$e^{-x}$")
    ax1.grid()
    ax2.plot(x_train2, y_train2, "o", ms=3, color="b")
    ax2.plot(x_test2, y_test2, "v", ms=3, color="y")
    ax2.plot(x_test2, pred2, "*", ms=3, color="r")
    ax2.set_title(f"Figure {num2}: Approximation of $|x|$")
    ax2.grid()
    ax3.plot(x_train3, y_train3, "o", ms=3, color="b")
    ax3.plot(x_test3, y_test3, "v", ms=3, color="y")
    ax3.plot(x_test3, pred3, "*", ms=3, color="r")
    ax3.set_title(f"Figure {num3}: Approximation of $x sin(\pi x)$")
    ax3.grid()
    ax3.legend(
        handles=[train_marker, test_marker, approx_marker],
        bbox_to_anchor=(1.05, 1),
        loc="upper left",
        title="Data",
        shadow=True,
        fancybox=True,
        borderaxespad=0,
        title_fontsize=12,
    )
    plt.show()

def gen_frame(true_list, pred_list, num):
    """Generates a dataframe of prediction accuracy of the three benchmark funcions.

    Args:
        true_list (list, np.array): Testing values.
        pred_list (list, np.array): Predicted values.
        num (int, float): Number of table

    Returns:
        pd.DataFrame: Prediction Accuracy.
    """
    accuracy = []
    for t, p in zip(true_list, pred_list):
        accuracy.append(mean_absolute_error(t, p))
        accuracy.append(mean_squared_error(t, p))
        accuracy.append(explained_variance_score(t, p))
        accuracy.append(r2_score(t, p))

    rslt = pd.DataFrame(
        [accuracy[i : i + 4] for i in range(0, len(accuracy), 4)],
        columns=[
            "Mean Absolute Error",
            "Mean Squared Error",
            "Explained Variance Score",
            "$R^2$ Score",
        ],
        index=["$e^{-x}$", "$|x|$", "$x sin(\pi x)$"],
    )
    return rslt.style.set_caption(
        f"Table {num}: Approximation-Accuracy of Keras Method for different functions"
    ).apply(highlight_min, subset=["Mean Absolute Error", "Mean Squared Error"])

# ---------------------------------------------------------------------- Keras Regressor

def _baseline_model():
    """Prepares neural system for the ``MultidimApprox`` object using Keraas 
    Sequential method. 

    Returns:
        function: Prepared neural system.
    """
    opt = Adam(learning_rate=1e-4)
    model = Sequential()
    model.add(Dense(9, input_dim=6, activation="relu", kernel_initializer="he_uniform"))
    model.add(Dense(6, activation="relu", kernel_initializer="he_uniform"))
    model.add(Dense(1, activation="relu", kernel_initializer="he_uniform"))
    model.compile(loss="mean_absolute_error", optimizer=opt)
    return model

class MultidimApprox:
    """Object that approximates happiness score using characteristics. 
    Uses ``_baseline_model`` as neural system.

    Args:
        path (str): Path of the dataset.
        out (str): Target variable.
    """
    def __init__(self, path, out):
        """Constructor Method. Prepares training and testing data.
        """
        self.path = path
        self.out = out

        if path.split(".")[-1] == "csv":
            self.data = pd.read_csv(path)
            data = self.data.select_dtypes(float)
        else:
            print("Data must be in csv format!")

        for i in data.isna().sum():
            if i != 0:
                data.dropna()
                print("There were NaN Values!")

        y = data[out].values
        X = data.drop(out, axis=1).values
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=0.33
        )

    def return_data(self):
        """Returns dataset information.

        Returns:
            pd.DataFrame: Information of the dataset.
        """
        return self.data

    def estimator(self, bs=5, vs=.1, epoch=1000):
        """Estimates testing data x as approximation for testing value y.

        Args:
            bs (int, optional): Batch size argument. Defaults to 5.
            vs (float, optional): Verbose argument. Defaults to 0.1.
            epoch (int, optional): Number of iterations. Defaults to 1000.
        """
        estimator = KerasRegressor(
            build_fn=_baseline_model, batch_size=bs, verbose=False
        )
        self.hist = estimator.fit(
            self.X_train, self.y_train, epochs=epoch, validation_split=vs
        )
        self.prediction = estimator.predict(self.X_test)

    def plt_first_rslt(self, num):
        """Plots accuracy of approximation.

        Plots Mean Absolute Error and Validation Loss by number of iterations to
        demonstrate its convergence. Plot accuracy of prediction.

        Args:
            num (int, float): Number of figures.
        """
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        ax1.plot(self.hist.history["loss"], label="Mean Absolute Error")
        ax1.plot(self.hist.history["val_loss"], label="Validation Loss")
        ax1.set_ylim([0, 2.5])
        ax1.set_xlabel("Epoch")
        ax1.set_ylabel("Error")
        ax1.set_title(f"Figure {num}: Epoch-Loss Trade off", y=1.105)
        ax1.legend(
            title="Loss",
            loc="upper center",
            bbox_to_anchor=(0.5, 1.1),
            ncol=3,
            title_fontsize=12,
            fancybox=True,
            shadow=True,
        )
        ax1.grid()
        ax2.plot(self.y_test, "o", label="True Values")
        ax2.plot(self.prediction, "x", label="Prediction")
        ax2.set_title(f"Figure {num + 0.1}: Prediciton", y=1.105)
        ax2.legend(
            title="Data",
            loc="upper center",
            bbox_to_anchor=(0.5, 1.1),
            ncol=3,
            fancybox=True,
            shadow=True,
            title_fontsize=12,
        )
        ax2.grid()
        plt.show()

    def plt_second_rslt(self, num1, num2):
        """Plots accuracy of approximation in terms of errors and
        prediction deviation. 

        Args:
            num1 (int, float): Number of first figure.
            num2 (int, float): Number of second figure.
        """
        accu = [
            mean_absolute_error(self.prediction, self.y_test),
            mean_squared_error(self.prediction, self.y_test),
            explained_variance_score(self.prediction, self.y_test),
            r2_score(self.prediction, self.y_test),
        ]
        error_df = pd.DataFrame(
            accu,
            index=[
                "Mean Absolute Error",
                "Mean Squared Error",
                "Explained Variance",
                "$R^2$ Score",
            ],
            columns=["Score"],
        )
        error = self.y_test - self.prediction

        fig = plt.figure(figsize=(17, 5))
        ax1 = fig.add_subplot(121)
        ax1.plot(error, "o", ms=4, label="Error Term")
        ax1.hlines(error.mean(), 0, 52, ls="--", lw=2, color="y", label="Mean Error")
        ax1.hlines(
            abs(error).mean(),
            0,
            52,
            ls="-.",
            lw=2,
            color="c",
            label="Mean Absolute Error",
        )
        ax1.hlines(
            (error ** 2).mean(),
            0,
            52,
            ls="dotted",
            lw=2,
            color="r",
            label="Mean Squared Error",
        )
        ax1.set_xlim([0, 50])
        ax1.set_ylim([-1.9, 1.6])
        ax1.grid()
        ax1.set_title(f"Figure {num1}: Error of multidimensional Approximation")
        ax1.legend(
            bbox_to_anchor=(1.05, 1),
            loc="upper left",
            title="Data",
            shadow=True,
            fancybox=True,
            borderaxespad=0,
            title_fontsize=12,
        )
        ax2 = fig.add_subplot(122)
        font_size = 10
        bbox = [0.4, 0, 0.2, 0.7]
        ax2.axis("off")
        ax2.set_title(f"Table {num2}: Accuracy Score Approximation", y=0.72)
        mpl_table = ax2.table(
            cellText=error_df.values.round(5),
            rowLabels=error_df.index,
            bbox=bbox,
            colLabels=error_df.columns,
        )
        mpl_table.auto_set_font_size(False)
        mpl_table.set_fontsize(font_size)
        plt.show()
