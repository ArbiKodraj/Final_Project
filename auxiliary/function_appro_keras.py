import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt

from matplotlib.lines import Line2D
from sklearn.model_selection import train_test_split
from itertools import product
from sklearn.metrics import (
    mean_absolute_error,
    mean_squared_error,
    explained_variance_score,
    r2_score,
)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.wrappers.scikit_learn import KerasRegressor

# ---------------------------------------------------------------------- Functions & Styling


def highlight_min(s):
    """Highlights the minimum in a Series yellow

    Args:
        s (pd.Series): Series that is to be highlighted.
    """
    is_max = s == s.min()
    return ["background-color: yellow" if v else "" for v in is_max]


def first_function(x):
    """``Benchmark function.``

    Args:
        x (int, float): Input.

    Returns:
        float: Output. :math:`e^{-x}`
    """
    return np.exp(-x)


def second_function(x):
    """``Benchmark function.``

    Args:
        x (int, float): Input.

    Returns:
        float: Output. :math:`|x|`
    """
    return abs(x)


def third_function(x):
    """Third benchmark function.

    Args:
        x (int, float): Input.

    Returns:
        float: Output. :math:`sin(πx)`
    """
    return x * np.sin(np.pi * x)


# ---------------------------------------------------------------------- Keras Sequential Method


class SeqMethod:
    """Generate neural network using Keras

    Objects that approximates benchmark functions ``first_function``,
    ``second_function``, and ``third_function`` using neural system
    generated by Keras Sequential object.

    Args:
        func (function): Benchmark function.
        nodes (int): Number of nodes. Defaults to 300.
        epochs (int): Number of epochs. Defaults to 1000.
    """

    def __init__(self, func, nodes=300, epochs=1000):
        """Constructor method."""
        self.func = func
        self.nodes = nodes
        self.epochs = epochs

    def seq_init(
        self,
        activator_out,
        activator_in="relu",
        n1_layer=20,
        n2_layer=18,
        n3_init=False,
        n3_layer=10,
    ):
        """Prepares Sequential model.

        Args:
            activator_out (str): Activation function for output layer.
            activator_in (str, optional): Activation function for input and hidden layers.
                Defaults to "relu".
            n1_layer (int, optional): Neuron number in first hidden layer. Defaults to 20.
            n2_layer (int, optional): Neuron number in second hidden layer. Defaults to 18.
            n3_init (bool, optional): Initiliazes third hidden layer if True.
                Defaults to False.
            n3_layer (int, optional): Neuron number in third hidden layer. Defaults to 10.
        """
        self.model = Sequential()
        self.model.add(
            Dense(
                n1_layer,
                input_dim=1,
                activation=activator_in,
                kernel_initializer="he_uniform",
            )
        )
        self.model.add(
            Dense(n2_layer, activation=activator_in, kernel_initializer="he_uniform")
        )
        if n3_init:
            self.model.add(
                Dense(
                    n3_layer, activation=activator_in, kernel_initializer="he_uniform"
                )
            )
        self.model.add(Dense(1, activation=activator_out))
        self.model.compile(optimizer="adam", loss="mae")

    def predict_test(self, a=-1, b=1, ts=0.4):
        """Predicts testings data using build Keras network.

        Args:
            a (int, optional): Lower bound of interval. Defaults to -1.
            b (int, optional): Upper bound of interval. Defaults to 1.
            ts (float, optional): Testing size 1-alpha. Defaults to 0.4.
        """
        X = np.linspace(a, b, self.nodes)
        y = self.func(X)
        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=ts
        )
        callback = EarlyStopping(
            monitor="val_loss", patience=100, restore_best_weights=True
        )
        self.model.fit(
            self.x_train,
            self.y_train,
            validation_data=(self.x_test, self.y_test),
            epochs=self.epochs,
            callbacks=[callback],
            verbose=False,
        )
        self.pred = self.model.predict(self.x_test)

    def return_data(self):
        """Returns training and testing data.

        Returns:
            np.array: Training (x,y) and testing (x,y) data.
        """
        return self.x_train, self.x_test, self.y_train, self.y_test, self.pred


def plt_side_by_side(
    num1,
    num2,
    num3,
    x_train1,
    y_train1,
    x_test1,
    y_test1,
    pred1,
    x_train2,
    y_train2,
    x_test2,
    y_test2,
    pred2,
    x_train3,
    y_train3,
    x_test3,
    y_test3,
    pred3,
):
    """Plots training, testing and predicted data of all three benchmark
    functions. Visualizes approximation accuracy of the ``SeqMethod``
    object.

    Args:
        num1 (int, float): Number of first figure.
        num2 (int, float): Number of second figure.
        num3 (int, float): Number of third figure.
        x_train1 (np.array): Training data x of first benchmark function.
        y_train1 (np.array): Training data y of first benchmark function.
        x_test1 (np.array): Testing data x of first benchmark function.
        y_test1 (np.array): Testing data y of first benchmark function.
        pred1 (np.array): Predicted data first benchmark function.
        x_train2 (np.array): Training data x of second benchmark function.
        y_train2 (np.array): Training data y of second benchmark function.
        x_test2 (np.array): Testing data x of second benchmark function.
        y_test2 (np.array): Testing data y of second benchmark function.
        pred2 (np.array): Predicted data of second benchmark function.
        x_train3 (np.array): Training data x of third benchmark function.
        y_train3 (np.array): Training data y of third benchmark function.
        x_test3 (np.array): Testing data x of third benchmark function.
        y_test3 (np.array): Testing data y of third benchmark function.
        pred3 (np.array): Predicted data of third benchmark function.
    """
    color = ["b", "y", "r"]
    marker = ["o", "v", "*"]
    labels = ["Testing Data", "Testing Data", "Keras Output"]
    handles = []
    for i in range(len(color)):
        handles.append(
            Line2D(
                [],
                [],
                color=color[i],
                marker=marker[i],
                linestyle="None",
                markersize=4,
                label=labels[i],
            )
        )

    figure, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))
    zips = [
        zip([x_train1, x_test1, x_test1], [y_train1, y_test1, pred1]),
        zip([x_train2, x_test2, x_test2], [y_train2, y_test2, pred2]),
        zip([x_train3, x_test3, x_test3], [y_train3, y_test3, pred3]),
    ]
    titles = [
        f"Figure {num1}: Approximation of " + "$e^{-x}$",
        f"Figure {num2}: Approximation of $|x|$",
        f"Figure {num3}: Approximation of $x sin(\pi x)$",
    ]
    for j, ax in enumerate([ax1, ax2, ax3]):
        for i, (x, y) in enumerate(zips[j]):
            ax.plot(x, y, marker[i], ms=3, color=color[i])
        ax.set_title(titles[j])
        ax.grid()
    ax3.legend(
        handles=handles,
        bbox_to_anchor=(1.05, 1),
        loc="upper left",
        title="Data",
        shadow=True,
        fancybox=True,
        borderaxespad=0,
        title_fontsize=12,
    )
    plt.show()


def gen_frame(true_list, pred_list, num):
    """Generates a dataframe of prediction accuracy of the three benchmark funcions.

    Args:
        true_list (list, np.array): Testing values.
        pred_list (list, np.array): Predicted values.
        num (int, float): Number of table

    Returns:
        pd.DataFrame: Prediction Accuracy.
    """
    accuracy = []
    for t, p in zip(true_list, pred_list):
        accuracy.append(mean_absolute_error(t, p))
        accuracy.append(mean_squared_error(t, p))
        accuracy.append(explained_variance_score(t, p))
        accuracy.append(r2_score(t, p))

    rslt = pd.DataFrame(
        [accuracy[i : i + 4] for i in range(0, len(accuracy), 4)],
        columns=[
            "Mean Absolute Error",
            "Mean Squared Error",
            "Explained Variance Score",
            "$R^2$ Score",
        ],
        index=["$e^{-x}$", "$|x|$", "$x sin(\pi x)$"],
    )
    return rslt.style.set_caption(
        f"Table {num}: Approximation-Accuracy of Keras Method for different functions"
    ).apply(highlight_min, subset=["Mean Absolute Error", "Mean Squared Error"])


# ---------------------------------------------------------------------- Keras Regressor


def _baseline_model():
    """Prepares neural system for the ``MultidimApprox`` object using Keraas
    Sequential method.

    Returns:
        function: Prepared neural system.
    """
    opt = Adam(learning_rate=1e-4)
    model = Sequential()
    model.add(Dense(9, input_dim=6, activation="relu", kernel_initializer="he_uniform"))
    model.add(Dense(6, activation="relu", kernel_initializer="he_uniform"))
    model.add(Dense(1, activation="relu", kernel_initializer="he_uniform"))
    model.compile(loss="mean_absolute_error", optimizer=opt)
    return model


class MultidimApprox:
    """Object that approximates happiness score using characteristics.
    Uses ``_baseline_model`` as neural system.

    Args:
        path (str): Path of the dataset.
        out (str): Target variable.
    """

    def __init__(self, path, out):
        """Constructor Method. Prepares training and testing data."""
        self.path = path
        self.out = out

        if path.split(".")[-1] == "csv":
            self.data = pd.read_csv(path)
            data = self.data.select_dtypes(float)
        else:
            print("Data must be in csv format!")

        for i in data.isna().sum():
            if i != 0:
                data.dropna()
                print("There were NaN Values!")

        y = data[out].values
        X = data.drop(out, axis=1).values
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=0.33
        )

    def return_data(self):
        """Returns dataset information.

        Returns:
            pd.DataFrame: Information of the dataset.
        """
        return self.data

    def estimator(self, bs=5, vs=0.1, epoch=1000):
        """Estimates testing data x as approximation for testing value y.

        Args:
            bs (int, optional): Batch size argument. Defaults to 5.
            vs (float, optional): Verbose argument. Defaults to 0.1.
            epoch (int, optional): Number of iterations. Defaults to 1000.
        """
        estimator = KerasRegressor(
            build_fn=_baseline_model, batch_size=bs, verbose=False
        )
        self.hist = estimator.fit(
            self.X_train, self.y_train, epochs=epoch, validation_split=vs
        )
        self.prediction = estimator.predict(self.X_test)

    def plt_first_rslt(self, num):
        """Plots Mean Absolute Error and Validation Loss by number of iterations to
        demonstrate its convergence. Plot accuracy of prediction.

        Args:
            num (int, float): Number of figures.
        """

        titles = ["Loss", "Data"]
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        axs_dict = {
            "ax1": zip(
                [self.hist.history["loss"], self.hist.history["val_loss"]],
                ["Mean Absolute Error", "Validation Loss"],
            ),
            "ax2": zip([self.y_test, self.prediction], ["True Values", "Prediction"]),
        }
        for e, (key, ax) in enumerate(zip(axs_dict.keys(), [ax1, ax2])):
            for i, j in axs_dict[key]:
                ax.plot(i, label=j, alpha=0.9)
            if e == 0:
                ax.set_ylim([0, 2.5])
                ax.set_xlabel("Epoch")
                ax.set_ylabel("Error")
            if e == 1:
                ax2.plot(self.y_test, "o", ms=3, color="blue")
                ax2.plot(self.prediction, "x", ms=3, color="orange")
            ax.legend(
                title=titles[e],
                loc="upper center",
                bbox_to_anchor=(0.5, 1.1),
                ncol=3,
                title_fontsize=12,
                fancybox=True,
                shadow=True,
            )
            ax.grid(True)
        ax1.set_title(f"Figure {num}: Epoch-Loss Trade off", y=1.105)
        ax2.set_title(f"Figure {num + 0.1}: Prediciton", y=1.105)
        plt.show()

    def plt_second_rslt(self, num1, num2):
        """Plots accuracy of approximation in terms of errors and
        prediction deviation.

        Args:
            num1 (int, float): Number of first figure.
            num2 (int, float): Number of second figure.
        """
        accu = [
            mean_absolute_error(self.prediction, self.y_test),
            mean_squared_error(self.prediction, self.y_test),
            explained_variance_score(self.prediction, self.y_test),
            r2_score(self.prediction, self.y_test),
        ]
        error_df = pd.DataFrame(
            accu,
            index=[
                "Mean Absolute Error",
                "Mean Squared Error",
                "Explained Variance",
                "$R^2$ Score",
            ],
            columns=["Score"],
        )
        error = self.y_test - self.prediction

        fig = plt.figure(figsize=(17, 5))
        ax1 = fig.add_subplot(121)
        ax1.plot(error, "o", ms=4, label="Error Term")
        ax1.hlines(error.mean(), 0, 52, ls="--", lw=2, color="y", label="Mean Error")
        ax1.hlines(
            abs(error).mean(),
            0,
            52,
            ls="-.",
            lw=2,
            color="c",
            label="Mean Absolute Error",
        )
        ax1.hlines(
            (error ** 2).mean(),
            0,
            52,
            ls="dotted",
            lw=2,
            color="r",
            label="Mean Squared Error",
        )
        ax1.set_xlim([0, 50])
        ax1.set_ylim([-1.9, 1.6])
        ax1.grid()
        ax1.set_title(f"Figure {num1}: Error of multidimensional Approximation")
        ax1.legend(
            bbox_to_anchor=(1.05, 1),
            loc="upper left",
            title="Data",
            shadow=True,
            fancybox=True,
            borderaxespad=0,
            title_fontsize=12,
        )
        ax2 = fig.add_subplot(122)
        font_size = 10
        bbox = [0.4, 0, 0.2, 0.7]
        ax2.axis("off")
        ax2.set_title(f"Table {num2}: Accuracy Score Approximation", y=0.72)
        mpl_table = ax2.table(
            cellText=error_df.values.round(5),
            rowLabels=error_df.index,
            bbox=bbox,
            colLabels=error_df.columns,
        )
        mpl_table.auto_set_font_size(False)
        mpl_table.set_fontsize(font_size)
        plt.show()


# ----------------------------------------------------------------------  Additional Keras implementation


def benchmark_function(x):
    """Benchmark function."""
    return x * np.sin(x)


class SeqApprox:
    """Object that approximates :func:`benchmark_function`
    for a wider range. Makes use of Keras Sequential Object.

        Args:
            func (function): Benchmark function.
            a (int, optional): Lower bound of interval. Defaults to 1.
            b (int, optional): Upper bound of interval. Defaults to 15.
            nodes (int, optional): Number of nodes. Defaults to 500.
            ts (float, optional): Testings size 1-α. Defaults to 0.4.

        Raises:
            ValueError: Interval must be strict positive, i.e., :math:`0 < a < b`
        """

    def __init__(self, func, a=1, b=15, nodes=500, ts=0.4):
        """Constructor method.
        """
        if not (a > 0 and b > a):
            raise ValueError("Let interval be strict positive!")
        x = np.linspace(a, b, nodes)
        y = func(x)
        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(
            x, y, test_size=ts
        )

    @staticmethod
    def _benchmark_model(layers=None, activations=None, optimizer="adam"):
        """Staticmethod that prepares the Sequential model.

        Args:
            layers (list, optional): Neuron number of each leayer except of input layer. Defaults to None.
            activations (list, optional): Used acitvation functions for defined layers. Defaults to None.
            optimizer (str, optional): Optimizer of model. Defaults to "adam".

        Raises:
            AssertionError: Length of layers and activations must be equal.

        Returns:
            function: Prepared Sequential model.
        """
        if activations is None:
            activations = ["relu", "relu", "sigmoid", "linear"]
        if layers is None:
            layers = [40, 80, 80, 1]
        assert len(layers) == len(activations)
        model = Sequential()
        for i, l in enumerate(layers):
            if i == 0:
                model.add(
                    Dense(
                        l,
                        input_dim=1,
                        activation=activations[i],
                        kernel_initializer="he_normal",
                    )
                )
            else:
                model.add(
                    Dense(l, activation=activations[i], kernel_initializer="he_normal")
                )
        model.compile(
            optimizer=optimizer,
            loss=["mean_squared_error"],
            metrics=["mean_squared_error"],
        )
        return model

    def plot_data(self, *arg, **kwargs):
        """Plots training and testing data.

        Raises:
            TypeError: Prediction must be an array (given by default).
            AssertionError: Size of testing data and prediction must be equal.
        """
        model = self._benchmark_model(*arg)
        self.hist = model.fit(self.x_train, self.y_train, **kwargs)
        prediction = model.predict(self.x_test)
        if not isinstance(prediction, np.ndarray):
            raise TypeError("prediction must be array type!")
        assert prediction.size == self.x_test.size == self.y_test.size
        labels = ["Training Data", "Testing Data", "Prediction"]
        markers = ["o", "v", "x"]
        plt.figure(figsize=(14, 7))
        for i, (x, y) in enumerate(
            zip(
                [self.x_train, self.x_test, self.x_test],
                [self.y_train, self.y_test, prediction],
            )
        ):
            plt.plot(x, y, markers[i], label=labels[i], ms=3)
        plt.legend()
        plt.title("Figure 13.4: Approximation of $x sin(x)$ for a wider range")
        plt.xlabel("x")
        plt.ylabel("B(x)")
        plt.grid(True)
        plt.show()

    def plot_history(self):
        """Plots history (MSE by Epochs) of model.
        """
        plt.figure(figsize=(12, 5))
        plt.plot(self.hist.history["loss"], label="Mean Squared Error")
        plt.title("Figure 13.5: Approximation Error by Epochs")
        plt.xlabel("Epochs")
        plt.ylabel("Error")
        plt.grid(True)
        plt.legend()
        plt.show()
